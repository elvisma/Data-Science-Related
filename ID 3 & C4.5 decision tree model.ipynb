{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树： ID 3 & C 4.5 算法  Based on Entropy (信息熵、信息增益、信息增益率）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 纯度：决策树的构造理解为寻找造纯度划分的过程，即解释为，让目标变量的分歧量最小。\n",
    "## 信息熵： 信息论中，随机离散时间出现的概率存在不确定性，香农引入信息熵的概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://static001.geekbang.org/resource/image/74/d5/741f0ed01c53fd53f0e75204542abed5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### p(i|t) 代表节点t为分类i的概率。Entropy越大， 代表着包含的信息量越大，不确定性也就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益 （ID 3 算法）： 划分可以带来纯度的提升，信息熵的下降。某个属性的信息增益最大， ID3算法则将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，比如我们把温度作为根节点。计算的过程中，会计算每一个子节点的归一化信息熵，按照每个子节点在父节点出现的概率，来结算这些子节点的信息熵，信息增益公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://static001.geekbang.org/resource/image/bf/34/bfea7626733fff6180341c9dda3d4334.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 公式D是父亲节点，Di是子节点，Gain（D,a）中的a作为D节点的属性选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益率 =信息增益/属性信息熵  (C4.5算法）\n",
    "## 悲观剪枝 （PEP）：防止过拟合，提高决策树的繁华能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://static001.geekbang.org/resource/image/d0/7a/d02e69930c8cf00c93578536933ad07a.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
